<document xmlns="http://cnx.rice.edu/cnxml" xmlns:md="http://cnx.rice.edu/mdml">
  <title>Climate Prediction</title>
  <metadata>
  <md:content-id>m31704</md:content-id><md:title>Climate Prediction</md:title>
  <md:abstract>This chapter provides an introduction to the use of advanced information technologies in climate research.</md:abstract>
  <md:uuid>f3c73c5b-c7ea-447b-89d1-e33a1b399f04</md:uuid>
</metadata>

<content>
    <para id="eip-317"><title>Key Concepts</title><list id="eip-id8434498">
<item>Climate prediction</item>
<item>Climate modeling</item>
</list></para><section id="eip-356"><title>Introduction</title>
    <para id="id1166594172616">Over 100 years ago Svante Arrhenius, who would go on to win the Nobel Prize for Chemistry, postulated that changes in levels of carbon dioxide in the atmosphere could affect global temperatures. We now know that a number of natural and industrial chemicals, including water vapour and carbon dioxide, affect the properties of the atmosphere. Levels of carbon dioxide, methane and nitrous oxide have increased markedly as a result of fossil fuel use, agriculture and land use change since the start of the industrial revolution. This past and projected future rise in emissions, coupled with the observed rise in global mean temperatures over the past three decades, has led to considerable concern about future climate change. For geoscientists seeking to understand how the global climate system operates, the challenge has been how to represent a system that is not fully accessible, because of the time and space constraints of experiments conducted on, and observations of, environmental systems. The solution has involved the development of numerical models to represent physical processes. </para>
    <para id="id1166595117116">Climate is the statistical average of the weather over long (30 year) periods of time. An old saying goes: “Climate is what you expect; weather is what you get.” Climate models have evolved over four decades from simple energy balance models to the massively complex global system models of today, which are largely extensions of models used for weather forecasting. This evolution has been enabled by the extraordinary development of computational power, largely in supercomputers but increasingly through dispersed applications, and the management of the correspondingly massive data sets. Not only have these developments provided far greater scope for more complex numerical models, the technology has fundamentally changed the scientific questions that can be posed. </para>
    <para id="id1166599710975">At the heart of such climate models are the mathematical equations representing geophysical processes. Such relationships are non-linear, necessitating a range of numerical methods to provide approximate iterative solutions to the equations. There is no one “best” climate model. Model components and subcomponents are combined to answer specific questions and at a time and space scale of interest to the user. For example, a global system model might include ocean, atmosphere, ecosystem and ice sheet components at coarse resolutions. A regional model might use finer numerical grids to resolve small-scale meteorological phenomena, but will need to use the outputs of the global model as a boundary condition to its more detailed study. </para>
    <para id="id6979424">The trade-off between model components and scale has typically reflected the computational efficiency of the model and its ability to include as much of the detailed physical processes as possible. However, even for models which incorporate detailed physical processes, the non-linear nature of the problem means that equations can only be solved approximately. There is inherent loss of information at scales below the averaging (grid) scales of models and through the process of parameterizing physical relationships within the model. As a result, confirmation that a complex climate model actually represents the underlying physical processes of the global climate is rather challenging; instead, the onus is on the modeller to establish a sufficient degree of confidence in the model through its ability to recreate observed data to a reasonable accuracy. This chapter first introduces the different approaches used by modellers for climate prediction, detailing the complexities of this endeavour. It concludes by considering the importance of collaborative working in development of predictive models and the future challenges facing climate science.</para>
</section>
    <section id="id7900753"><title>Predicting the future? Model Ensembles</title>
    <para id="id1166599258252">Attempting to predict the future has profound implications for model development and application. Until very recently, information from climate models about possible future climates has been presented as a scenario or projection, without specified probabilities. This has reflected the difficulty of managing the core uncertainties associated with climate modelling: </para>
    <list id="id1166599255279" list-type="bulleted">
      <item>Changing boundary conditions: these are the factors affecting the climate system that are treated as separate from, or outside, the climate system. These include changes in solar output, volcanic eruptions and human factors, such as levels of emissions of greenhouse gases</item>
      <item>The natural internal variability of the global climate system: the global climate system is chaotic, which means that very small changes in one location and at one point in time can lead to large differences in other locations at a future point in time</item>
      <item>The extent to which the models accurately represent (parameterize) the physical processes of the climate system; in other words our understanding of the component parts of the global climate system and how they respond to change</item>
    </list>
    <para id="id7867287">With increasing demands from the public and private sectors for information to manage future changes in climate, and with enhanced computational power, climate modellers can now begin to explore this range of uncertainty. Different approaches exist for developing probabilistic climate predictions. One relies on brute force, based on large <emphasis effect="italics">ensembles</emphasis> of simulations from computationally efficient models. This approach carries out large numbers of model runs in which model parameters are varied within their current range of uncertainty. Model parameterizations which fail to replicate existing climate observations are rejected, with the remainder used to explore future climate scenarios. This approach is complemented by continuous improvement in model representations of physical processes and higher resolution data, which improves the parameterizations – the model representation of physical processes. The second approach for developing probabilistic predictions relies on “expert judgement”, drawn from small <emphasis effect="italics">ensembles</emphasis> of state-of-the-art models. </para>
    <para id="id1166596083468">An <emphasis effect="italics">ensemble</emphasis> consists of many simulations run with a specific climate model, each one slightly different from the rest. The uncertainty associated with natural climate variability is studied using “initial condition” <emphasis effect="italics">ensembles</emphasis>, which vary the distribution of temperature, wind, humidity and other factors at the beginning of the simulation. The uncertainty associated with the model boundary conditions is studied using <emphasis effect="italics">ensembles</emphasis> with different scenarios for human-induced or natural greenhouse gas emissions. These seek to examine the full range of possible boundary conditions of, for example, future global greenhouse gas emissions from society under different economic futures. The final source of uncertainty reflects the quality of the model representation of the climate; this is studied by using ensembles of different climate models. This approach assumes that the available models from climate modelling centres capture the full range of plausible behaviour, though this is unlikely to be the case. This source of uncertainty remains least studied, and potentially most important.</para>
    <para id="id1166596039938">These uncertainties, associated with natural variability, boundary conditions and model representation, are not independent of each other. To explore the full “parameter” space of the models requires the integration of these different model runs into a grand ensemble of ensembles! </para>
    <para id="id1166595010452">Obviously, these climate model ensembles are extraordinarily computationally-intense. A global climate model might typically need to solve its equations for each grid point on a simulated 30 minute time-step. While weather forecasting might require simulated time to be of the order of a few days, climate forecasting requires simulated time to be of the order of decades. So each simulated time-step in the model must be repeated tens of thousands of times. As a result, accurately modelling atmospheric processes alone can require high performance computer runs of several days. Because of these constraints, until recently complex climate models have only been run on supercomputers. With the need for grand ensembles of model runs to explore climate uncertainty more fully, the growth of distributed computing approaches is appealing. </para>
    <para id="id3021071">A public example of a grand ensemble has been the work of <emphasis effect="italics">climateprediction.net</emphasis>, which used distributed computing resources of the general public to run ensembles of a version of the UK Meteorological Office Unified Climate Model to examine the implications of doubling levels of carbon dioxide in the atmosphere. Members of the public donate spare computing capacity on their personal computers to do one or more of the simulations. Public interest has been staggering. Over 100,000 people from 150 countries have taken part to date. More than 70,000 simulations of the climate model have been completed. In contrast, at the time climateprediction.net started, the largest model ensemble reported in the literature was 53. </para>
    <para id="id1166596112640">Outputs from these model ensembles provide a snapshot of the range of uncertainty associated with future climate – whether internal model uncertainty or external uncertainty associated with future global greenhouse gas emissions. Although these outputs have been termed probabilistic predictions, they are not objective probabilities. Instead they are subjective probabilities, based on the quality and availability of information at the present time. They cannot capture all uncertainties about future climates: different climate models produce different results based on the same forcing scenarios. For example, one model might suggest that rainfall increases across the Amazon; another might suggest that rainfall will decrease. Different models are better at representing different elements of the climate system. When many model outputs converge, we might have more confidence that we are seeing a robust result from the many possible representations of the climate system.</para>
    <para id="id1166593606406">In this way, “probabilistic” predictions do not reduce uncertainty in future climates, but they do make the range of possible futures more transparent. It should, in theory, make decision making more transparent. One recent example of such probabilistic predictions is the publication of the UK Climate Impacts Programme (UKCIP) climate scenarios, which provide probabilistic projections for seven overlapping 30 year time slices through until 2099 for 25km x 25km land grid squares in the UK.</para>
</section>
    <section id="id1166583686231"><title>Summary: The Importance of Collaborations and Future Challenges for Climate Research</title>
    <para id="id1166607492328">Twenty years ago, it was not uncommon for a doctoral graduate student to be expected to produce a numerical model of some element of the climate system, for example an ice sheet or an ecosystem model, track down the necessary data to test the model, and use the model to examine a science question. Typically, the model details (parameter values; computer code) would not be published, and only sparse elements of the model outputs would be made available through published papers. To all intents and purposes, these model results cannot be replicated, since the model is likely to be discarded or developed through time, with poor notification of model versions and parameter values. </para>
    <para id="id1166591209485">In time, modellers recognised the importance of benchmarking models against other models, to examine the strengths and weaknesses of their particular representation of the climate system. Funding was made available to bring together modellers and to ensure effective data and model management. More recently, community-wide models of different elements of the global climate system have been developed. For example, in the UK, the GLIMMER ice sheet model is available as a resource to the academic community. This model captures the learning developed over many years by different modelling groups across the UK. A new doctoral graduate student who is exploring science questions about ice sheets is likely to use this as the starting point, perhaps with the aim of improving physical processes within the model, or using the model to answer new science questions. These community model developments, enabled by distributed access to computer models and more effective model and data management, have changed the way modellers work together. Instead of a largely individual approach, collegiate approaches are now the norm.</para>
    <para id="id1166596412374">e-Research methods have had a fundamental impact on the way in which climate science is undertaken. These methods have changed how individuals and modelling groups work together. They have changed the very science questions that can be posed. However, the very success of high performance or distributed computing to produce colourful ensemble model outputs has also disguised critical questions about what models can usefully offer and how the outputs are used by decision makers and politicians. To those outside the modelling community, “probabilistic predictions” might well be assumed to be objective probabilities of future events, rather than subjective assessments based on incomplete information. Such a perception will affect the decisions that are taken about managing future climate impacts. Yet, climate models are not truth machines; they are inherently partial. In practice, there is an asymmetry between explanation and prediction of complex systems. Satisfactory explanation of the future is possible even when absolute prediction is impossible. </para>
    <para id="id1166593607666">Separately, extensive work by behavioural economists has shown that humans are inherently poor at calculating and managing probabilities when making monetary decisions. Yet the output of these ensemble model runs shows a wide range of probabilistic outcomes, from futures with little change to futures with catastrophic change. Taking the next step and enabling more effective decision making on the basis of these model outputs remains challenging. </para>
    <para id="id1166595136821">While computer-enabled methods of research may not be able to address these problems of human decision making, they have enhanced climate science through the development of models that expand our knowledge of a range of possible future climates that could occur based on different variables. </para>
</section>
  </content>
</document>